[{
    "title": "Satellite imagery classification - III",
    "date": "",
    "description": "Classification with the help of the Python eobox package.",
    "body": "Introduction Context and goal This is the third and last part of a blog post series about using remote sensing data to classify the earth\u0026rsquo;s surface. In this post we will finally walk through the typical steps it takes to classify remote sensing images with a supervised classifier to derive a land use/land cover map.\nWe will make use of the remote sensing data from the first part of the series and the OpenStreetMap (OSM) data from the second part of the series.\nWith these ingredients, a machine learning model can learn patterns between feature vectors and their respective target categories. Then the model can be used to estimate the target category of other feature vectors for creating the land use/land cover map or for model validation by comparing estimated and known target categories. This is the goal of this post.\nProcessing steps More specifically, in this post, we will walk through the following steps:\n  Create a reference dataset. Therefore, we need to extract pixel values of the Landsat raster data where it overlays with the polygons of the OSM vector data. Then, we can create a simple two-dimensional table, or reference dataset, which contains, among other information, the labels (land use/land cover information) and features (raster pixel values).\n  Create a training and test dataset. We will do this by splitting the reference dataset.\n  Train a classification model. We will do this with the training set and a random forest classifier.\n  Validate a classification model. We will do this by comparing the known labels of the test dataset with the labels predicted with the trained model.\n  Predict the image data and derive a map. We will do this by predicting all pixels values of the raster, not only the extracted ones.\n  eobox For the earth observation specific tasks we will use eobox, a toolbox for processing earth observation data with Python which I started building for learning and practicing purposes. It contains several smaller helpful utilities that might be useful for someone working with remote sensing data in Python. However, I believe that it really might be able to make your life easier if you find yourself in the following situation:\n  You have many large raster files stored as a single-layer GDAL-readable file.\n  The raster files have identical extents, pixel resolution alignment.\n  You want to do pixel-based processing (e.g. over the spectral and/or temporal dimension) and need all pixel values of the whole stack in memory.\n  You want to process the data locally (on your notebook or a server) but the whole stack of the full raster extent does not fit in memory.\n  You do not want to physically split and/or stack the data and you do not want to write boilerplate code to perform chunk-wise processing of the raster stacks.\n  You are patient and okay to wait for more complex processing steps because the package is not optimized for computational performance.\n  The setup and/or learning effort for using another more professional framework is too high and not required for the scope of your problem.\n  In this post, we are processing a raster dataset that would easily fit in any current standard notebook. However, the same code works also on raster datasets (such as the 10,000 by 10,000 pixel Sentinel-2 tiles) that are larger than the computer\u0026rsquo;s memory because internally the computations are done on spatial chunks. Without the need of writing a lot of boilerplate code, this enables us to run tasks without such as the creation of temporal statistical metrics, virtual time series by linear temporal interpolation (see EOCubeSceneCollection - from level-2 to level-3 products), and the prediction of a sklearn model on large feature stacks. Additionally, it is easy to write custom functions that can be developed with a spatial chunk of the data and then applied over the whole data EOCubeSceneCollection - custom functions.\nHowever, before using and building on the package, it is important to be aware that it is only a small personal free time project. It is not as mature, performant, supported, stable, and full of features as many professional and/or community-driven framework out there. The following is a non-exhaustive list projects in Python or with a Python API for earth observation data that I find very interesting:\n  eo-learn\n  xcube\n  Open Data Cube\n  gdalcubes\n  RasterFrames\n  Pangeo\n  OpenEO\n  EarthPy\n  xarray\n  Additionally, you might find more interesting links in the Awesome Geospatial list of geospatial analysis tools.\nIn this landscape of tools, eobox might be just fine to solve your problem and, compared to some of these frameworks, it might be easier to get started with it. It does not require to set up a database and ingesting or indexing the data (like the Open Data Cube) or to store the data in numpy arrays (like in case of the eo-learn framework). gdalcubes is a very promising project with even more flexibility but it is currently in a very early stage of development and has (so far) only R and not Python bindings. But writing a comprehensive analysis of the scope, strengths, and weaknesses of these projects is a very interesting topic but not the scope of this post \u0026ndash; please leave a note in the comments if you are aware of such a comparative analysis. Instead, let us now walk through the steps it takes to classify remote sensing images with a supervised classifier to derive a land use/land cover map.\nSupervised classification of remote sensing data Libraries We will use the following libraries and modules in this post:\nimport geopandas as gpd import matplotlib.pyplot as plt import numpy as np import pandas as pd from pathlib import Path import rasterio import seaborn as sns from shapely.geometry.polygon import Polygon from shapely.geometry.multipolygon import MultiPolygon from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report, \\ confusion_matrix from sklearn.model_selection import train_test_split from eobox.raster import extract, \\ load_extracted, \\ add_vector_data_attributes_to_extracted, \\ EOCube from eobox.ml import plot_confusion_matrix, \\ predict_extended Create a reference dataset As described above, a reference dataset is required for supervised classification. A reference dataset contains the target class information, here land use/land cover information from the OSM vector data, and the feature vectors, i.e. the pixel values of the Landsat raster data.\nWe will use the extract function imported above from the eobox package to perform the extraction:\nextract(src_vector: str, burn_attribute: str, src_raster: list, dst_names: list, dst_dir: str, dist2pb: bool = False, dist2rb: bool = False, src_raster_template: str = None, gdal_dtype: int = 4, n_jobs: int = 1) -\u0026gt; int You can find the latest description of the function and arguments in function\u0026rsquo;s documentation but we will also work through it in this section.\nAs we can see from the function signature the function returns an integer, i.e. an exit code of 0 if the process was successful and 1 otherwise. That means, the function does not directly return the reference dataset but stores the extracted values as NumPy binary files in the directory given via dst_dir. Later we will load the data with the function load_extracted.\nBefore we can call the function it is necessary to do some preparatory work for passing the right data to the arguments src_vector, src_raster, and dst_names.\nThe documentation of src_vector says:\nFilename of the vector dataset. Currently, it must have the same CRS as the raster.\nAnd about the related burn_attribute we can read:\nName of the attribute column in the src_vector dataset to be stored with the extracted data. This should usually be a unique ID for the features (points, lines, polygons) in the vector dataset. Note that this attribute should not contain zeros since this value is internally used for pixels that should not be extracted, or, in other words, that do not overlap with the vector data.\nThus, we will create a new dataset from the OSM vector dataset, which is reprojected and has an attribute containing a non-zero polygon ID.\n# path of the original OSM vector dataset  path_osm_4326 = \u0026#34;./data_federsee/osm_feedersee_cleansed_4326.geojson\u0026#34; # path of the new vector dataset that can be passed to  # src_vector path_osm = \u0026#34;./data_federsee/osm_feedersee_cleansed_32632.geojson\u0026#34; First, we read the file, reproject it the raster CRS and create the non-zero polygon ID and a class ID for the level-1 classes along with colors for the classes, which we will use for plots later.\nosm = gpd.read_file(path_osm_4326) osm = osm.to_crs(epsg=32632) # reproject in the raster CRS osm[\u0026#34;pid\u0026#34;] = range(1, osm.shape[0] + 1) # polygon ids - do not use 0 osm[\u0026#34;cid_l1\u0026#34;] = osm[\u0026#34;lun_l1\u0026#34;].astype(\u0026#34;category\u0026#34;).cat.codes + 1 class_lookup_l1 = osm[[\u0026#34;lun_l1\u0026#34;, \u0026#34;cid_l1\u0026#34;]] \\ .groupby(\u0026#34;cid_l1\u0026#34;) \\ .first() \\ .reset_index() class_lookup_l1[\u0026#34;color\u0026#34;] = [\u0026#39;#e31a1c\u0026#39;, \u0026#39;#ff7f00\u0026#39;, \u0026#39;#33a02c\u0026#39;, \u0026#39;#b2df8a\u0026#39;, \u0026#39;#b15928\u0026#39;, \u0026#39;#1f78b4\u0026#39;, \u0026#39;#a6cee3\u0026#39;] class_lookup_l1.to_csv(\u0026#34;./data_federsee/class_lookup_l1.csv\u0026#34;) class_lookup_l1  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  cid_l1 lun_l1 color     0 1 buildup #e31a1c   1 2 farmland #ff7f00   2 3 forest #33a02c   3 4 grassland #b2df8a   4 5 orchard #b15928   5 6 water #1f78b4   6 7 wetland #a6cee3     In the case here, we also need to convert the geometry type of all polygons to the MultiPolygon type.\nosm[\u0026#34;geometry\u0026#34;] = [MultiPolygon([geom]) if type(geom) == Polygon \\ else geom for geom in osm[\u0026#34;geometry\u0026#34;]] assert osm[\u0026#34;geometry\u0026#34;].type.unique() == \u0026#34;MultiPolygon\u0026#34; Else we will get the following error later during extraction.\nValueError: Record\u0026#39;s geometry type does not match collection schema\u0026#39;s geometry type: \u0026#39;MultiPolygon\u0026#39; != \u0026#39;Polygon\u0026#39; Finally, we can write the new vector dataset to a file.\nosm.to_file(path_osm, driver=\u0026#39;GeoJSON\u0026#39;) src_raster is the list of file paths of the single-band raster files from which to extract the pixel values.\npaths_landsat = Path(\u0026#34;./data_federsee/l8_aoi\u0026#34;).rglob(\u0026#34;**/*.TIF\u0026#34;) paths_landsat = sorted(list(paths_landsat)) And dst_names the list of names corresponding to src_raster. These names will be used for the names of the NumPy binary files and will be the column names when the data is loaded with load_extracted. Here, the following names contain enough information to be uniquely identifiable.\n# dst_names names_landsat = [f\u0026#34;{fp.stem[17:25]}_{fp.stem[-2::]}\u0026#34; \\ for fp in paths_landsat] Let\u0026rsquo;s keep all relevant information about the Landsat layers in a DataFrame:\nlandsat_layers = pd.DataFrame( { \u0026#34;uname\u0026#34;: names_landsat, \u0026#34;date\u0026#34;: [pd.datetime(int(name[:4]), int(name[4:6]), int(name[6:8])) for name in names_landsat], \u0026#34;band\u0026#34;: [name.split(\u0026#34;_\u0026#34;)[1] for name in names_landsat], \u0026#34;path\u0026#34;: paths_landsat, } ) landsat_layers.to_csv(\u0026#34;./data_federsee/landsat_layers.csv\u0026#34;) landsat_layers  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  uname date band path     0 20190412_B1 2019-04-12 B1 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   1 20190412_B2 2019-04-12 B2 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   2 20190412_B3 2019-04-12 B3 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   3 20190412_B4 2019-04-12 B4 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   4 20190412_B5 2019-04-12 B5 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   5 20190412_B6 2019-04-12 B6 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   6 20190412_B7 2019-04-12 B7 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   7 20190818_B1 2019-08-18 B1 data_federsee/l8_aoi/LC08_L1TP_194026_20190818...   8 20190818_B2 2019-08-18 B2 data_federsee/l8_aoi/LC08_L1TP_194026_20190818...   9 20190818_B3 2019-08-18 B3 data_federsee/l8_aoi/LC08_L1TP_194026_20190818...   10 20190818_B4 2019-08-18 B4 data_federsee/l8_aoi/LC08_L1TP_194026_20190818...   11 20190818_B5 2019-08-18 B5 data_federsee/l8_aoi/LC08_L1TP_194026_20190818...   12 20190818_B6 2019-08-18 B6 data_federsee/l8_aoi/LC08_L1TP_194026_20190818...   13 20190818_B7 2019-08-18 B7 data_federsee/l8_aoi/LC08_L1TP_194026_20190818...   14 20190903_B1 2019-09-03 B1 data_federsee/l8_aoi/LC08_L1TP_194026_20190903...   15 20190903_B2 2019-09-03 B2 data_federsee/l8_aoi/LC08_L1TP_194026_20190903...   16 20190903_B3 2019-09-03 B3 data_federsee/l8_aoi/LC08_L1TP_194026_20190903...   17 20190903_B4 2019-09-03 B4 data_federsee/l8_aoi/LC08_L1TP_194026_20190903...   18 20190903_B5 2019-09-03 B5 data_federsee/l8_aoi/LC08_L1TP_194026_20190903...   19 20190903_B6 2019-09-03 B6 data_federsee/l8_aoi/LC08_L1TP_194026_20190903...   20 20190903_B7 2019-09-03 B7 data_federsee/l8_aoi/LC08_L1TP_194026_20190903...     The rest of the arguments are quite easy to specify. We define a destination directory for the extracted data and change arguments dist2pb to True. For the rest of the arguments, we are fine with the defaults.\nThe arguments have the following meaning:\ndst_dir {str}: Directory to store the data to.\ndist2pb {bool}: Create an additional auxiliary layer containing the distance to the closest polygon border for each extracted pixel. Defaults to False.\ndist2rb {bool}: Create an additional auxiliary layer containing the distance to the closest raster border for each extracted pixels. Defaults to False.\nsrc_raster_template {str}: A template raster to be used for rasterizing the vectorfile. Usually the first element of src_raster. (default: {None})\ngdal_dtype {int}: Numeric GDAL data type, defaults to 4 which is UInt32. See https://github.com/mapbox/rasterio/blob/master/rasterio/dtypes.py for useful look-up tables.\nn_jobs {int}: Number of parallel processors to be used for extraction. -1 uses all processors. Defaults to 1.\nNow we can perform the extraction.\ndir_refset = \u0026#34;./data_federsee/refset\u0026#34; extract( src_vector = path_osm, burn_attribute = \u0026#34;pid\u0026#34;, src_raster = landsat_layers[\u0026#34;path\u0026#34;], dst_names = landsat_layers[\u0026#34;uname\u0026#34;], dst_dir = dir_refset, dist2pb=True, dist2rb=False, src_raster_template = None, gdal_dtype = 4, n_jobs = 1 ) 100%|██████████| 176/176 [00:00\u0026lt;00:00, 34128.41it/s] 100%|██████████| 21/21 [00:00\u0026lt;00:00, 863.53it/s] 0  As a result, the following files are created in the destination directory.\nlist(Path(dir_refset).glob(\u0026#34;*\u0026#34;)) [PosixPath('data_federsee/refset/20190903_B6.npy'), PosixPath('data_federsee/refset/20190412_B4.npy'), PosixPath('data_federsee/refset/aux_coord_x.npy'), PosixPath('data_federsee/refset/20190412_B7.npy'), PosixPath('data_federsee/refset/20190412_B2.npy'), PosixPath('data_federsee/refset/20190412_B3.npy'), PosixPath('data_federsee/refset/20190903_B7.npy'), PosixPath('data_federsee/refset/20190818_B4.npy'), PosixPath('data_federsee/refset/20190818_B7.npy'), PosixPath('data_federsee/refset/20190903_B2.npy'), PosixPath('data_federsee/refset/burn_attribute_rasterized_pid.tif'), PosixPath('data_federsee/refset/20190412_B5.npy'), PosixPath('data_federsee/refset/20190903_B4.npy'), PosixPath('data_federsee/refset/aux_vector_dist2pb.npy'), PosixPath('data_federsee/refset/20190818_B3.npy'), PosixPath('data_federsee/refset/20190412_B6.npy'), PosixPath('data_federsee/refset/20190818_B2.npy'), PosixPath('data_federsee/refset/20190903_B5.npy'), PosixPath('data_federsee/refset/20190818_B5.npy'), PosixPath('data_federsee/refset/aux_coord_y.npy'), PosixPath('data_federsee/refset/20190412_B1.npy'), PosixPath('data_federsee/refset/20190903_B3.npy'), PosixPath('data_federsee/refset/aux_vector_dist2pb.tif'), PosixPath('data_federsee/refset/20190903_B1.npy'), PosixPath('data_federsee/refset/20190818_B6.npy'), PosixPath('data_federsee/refset/20190818_B1.npy'), PosixPath('data_federsee/refset/aux_vector_pid.npy')]  As we can see we get one NumPy binary file per raster layer and additional auxiliary information: the coordinates of the pixels (aux_coord_x, aux_coord_y), the distance to the polygon border (aux_vector_dist2pb) and the polygon ID (aux_vector_pid). The GeoTIFFs are intermediate data holding the respective information as a raster.\nStill, for building and evaluating the classification model we need the target class information along with the pixel values. Additionally, sometimes we have other information stored in the vector data that we need for the analysis. We could later join the data in by using the polygon ID, however, for fast and easy access, we can also store that data as NumPy binary files along with the others. As you can see from the function\u0026rsquo;s feedback so far only numeric columns are supported.\nadd_vector_data_attributes_to_extracted( ref_vector=path_osm, pid=\u0026#39;pid\u0026#39;, dir_extracted=dir_refset, overwrite=False) Skipping column landuse - datatype 'object' not (yet) supported. Skipping column leaf_type - datatype 'object' not (yet) supported. Skipping column natural - datatype 'object' not (yet) supported. Skipping column water - datatype 'object' not (yet) supported. Skipping column wetland - datatype 'object' not (yet) supported. Skipping column lun - datatype 'object' not (yet) supported. Skipping column lun_l1 - datatype 'object' not (yet) supported. Skipping column color - datatype 'object' not (yet) supported.  Note that all the data derived from the vector data are prefixed with *aux_vector_*. This is true for the polygon ID and distance to the polygon border as well as for the additional attributes derived by add_vector_data_attributes_to_extracted.\nlist(Path(dir_refset).glob(\u0026#34;aux_vector_*.npy\u0026#34;)) [PosixPath('data_federsee/refset/aux_vector_cid_l1.npy'), PosixPath('data_federsee/refset/aux_vector_area_m2.npy'), PosixPath('data_federsee/refset/aux_vector_dist2pb.npy'), PosixPath('data_federsee/refset/aux_vector_pid.npy')]  Now we can use load_extracted to load the data into a pandas DataFrame. You can load the data you want by an appropriate file pattern or list of file patterns. Here we use to files patterns that will load all data but makes sure that the auxiliary data is stored in the first columns of the DataFrame.\nrefset = load_extracted(src_dir=dir_refset, patterns=[\u0026#39;aux_*.npy\u0026#39;, \u0026#39;2019*.npy\u0026#39;], sort=True) refset.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  aux_coord_x aux_coord_y aux_vector_area_m2 aux_vector_cid_l1 aux_vector_dist2pb aux_vector_pid 20190412_B1 20190412_B2 20190412_B3 20190412_B4 ... 20190818_B5 20190818_B6 20190818_B7 20190903_B1 20190903_B2 20190903_B3 20190903_B4 20190903_B5 20190903_B6 20190903_B7     0 541470.0 5328390.0 6.111917e+06 3 1.000000 4 9849 8872 7829 7127 ... 16736 8350 6183 8866 7960 7155 6278 15606 7966 6091   1 541500.0 5328390.0 6.111917e+06 3 1.414214 4 9806 8809 7778 6967 ... 16108 8155 6099 8864 7947 7123 6273 14787 7619 5946   2 541530.0 5328390.0 6.111917e+06 3 2.236068 4 9767 8787 7724 6905 ... 14122 7252 5805 8841 7929 7056 6227 13124 6924 5708   3 541560.0 5328390.0 6.111917e+06 3 3.162278 4 9750 8787 7687 6872 ... 12640 7004 5752 8828 7911 7011 6194 11958 6632 5630   4 541590.0 5328390.0 6.111917e+06 3 4.123106 4 9780 8821 7725 6943 ... 13200 7258 5846 8813 7899 7044 6207 12235 6843 5719    5 rows × 27 columns\n With that data, we can already perform initial explorative data analysis (EDA). For example, we can compare the class distribution of single features (boxplots) or bivariate distributions (scatterplots).\nax = refset.boxplot(by=\u0026#39;aux_vector_cid_l1\u0026#39;, column=[\u0026#39;20190818_B5\u0026#39;], grid=False, figsize=(10, 5)) refset[\u0026#34;aux_added_color\u0026#34;] = refset[\u0026#34;aux_vector_cid_l1\u0026#34;] \\ .map(class_lookup_l1.set_index(\u0026#34;cid_l1\u0026#34;)[\u0026#34;color\u0026#34;]) ax = refset \\ .plot.scatter(x=\u0026#34;20190818_B5\u0026#34;, y=\u0026#34;20190903_B4\u0026#34;, s=1, c=refset[\u0026#34;aux_added_color\u0026#34;], figsize=(10, 5)) Of course, there is much more to investigate here but details are not the focus of this post.\nCreate a training and test dataset Even though our data is now in a tabular form we should not forget, that we are working with spatial data where spatially close samples are more likely to be similar. This should be considered when a dataset is split into a training and test dataset since they should be relatively independent of each other. Therefore we generate two datasets that are spatially disjointed on polygon level, i.e. we assure that different pixels from the same polygon do not appear in both datasets. Thus, we derive a DataFrame with one sample per polygon and split on polygon-level.\n# dataframe with one row per polygon, polygon ID and class ID aux_polygon = refset[[\u0026#34;aux_vector_pid\u0026#34;, \u0026#34;aux_vector_cid_l1\u0026#34;]] \\ .groupby(\u0026#34;aux_vector_pid\u0026#34;) \\ .first() \\ .reset_index() # polygon IDs of the train and test set pids_train, pids_test, _, _ = train_test_split( aux_polygon[[\u0026#34;aux_vector_pid\u0026#34;]], aux_polygon[\u0026#34;aux_vector_cid_l1\u0026#34;], stratify=aux_polygon[\u0026#34;aux_vector_cid_l1\u0026#34;], test_size=0.5, random_state=11) # pixels belonging to training and test polygons respectively trainset = refset[ refset[\u0026#34;aux_vector_pid\u0026#34;].isin(pids_train[\u0026#34;aux_vector_pid\u0026#34;])] testset = refset[ refset[\u0026#34;aux_vector_pid\u0026#34;].isin(pids_test[\u0026#34;aux_vector_pid\u0026#34;])] # overview: number of polygons \u0026amp; pixels in training and test set pd.DataFrame( { \u0026#34;n_pixels\u0026#34;: refset[\u0026#34;aux_vector_cid_l1\u0026#34;] \\ .value_counts().sort_index(), \u0026#34;n_pixels_train\u0026#34;: trainset[\u0026#34;aux_vector_cid_l1\u0026#34;] \\ .value_counts().sort_index(), \u0026#34;n_pixels_test\u0026#34;: testset[\u0026#34;aux_vector_cid_l1\u0026#34;] \\ .value_counts().sort_index(), \u0026#34;n_polygons\u0026#34;: refset \\ .groupby(\u0026#34;aux_vector_cid_l1\u0026#34;) \\ .apply(lambda x: x[\u0026#34;aux_vector_pid\u0026#34;].nunique()), \u0026#34;n_polygons_train\u0026#34;: trainset \\ .groupby(\u0026#34;aux_vector_cid_l1\u0026#34;) \\ .apply(lambda x: x[\u0026#34;aux_vector_pid\u0026#34;].nunique()), \u0026#34;n_polygons_test\u0026#34;: testset \\ .groupby(\u0026#34;aux_vector_cid_l1\u0026#34;) \\ .apply(lambda x: x[\u0026#34;aux_vector_pid\u0026#34;].nunique()), } )  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  n_pixels n_pixels_train n_pixels_test n_polygons n_polygons_train n_polygons_test     1 1268 829 439 25 12 13   2 6435 2935 3500 46 23 23   3 8762 7494 1268 28 14 14   4 3856 1281 2575 48 24 24   5 30 23 7 3 2 1   6 1634 11 1623 3 1 2   7 10930 9808 1122 18 9 9     We can see that, due to the polygon-based split, we have an extremely unbalanced number of pixels per class and training/test dataset. However, as before, we ignore this detail here and go on with the next step in our walk-through.\nTrain a classification model With the training dataset, it is easy to train a simple classification model. We will train a Random Forest classifier here since, compared to other classifiers, it usually performs well even without preprocessing the features (e.g. scaling) or tuning the hyper-parameters.\nrf_clf = RandomForestClassifier(n_estimators=200, oob_score=True, n_jobs=-1, random_state=123) rf_clf = rf_clf.fit(trainset[landsat_layers[\u0026#34;uname\u0026#34;]], trainset[\u0026#34;aux_vector_cid_l1\u0026#34;]) Validate a classification model To perform a first validation, or accuracy assessment, of the model we first need to derive the model predictions for the samples of the test dataset. Then we can compare these values with the known target class information. Precision, recall F1-score and the confusion matrix are frequently reported for accuracy assessment. Note that we switch the axes of the confusion matrix since in remote sensing the actual class memberships are more frequently reported in the columns and the predictions in the rows.\ny_pred = rf_clf.predict(testset[landsat_layers[\u0026#34;uname\u0026#34;]]) y_test = testset[\u0026#34;aux_vector_cid_l1\u0026#34;] print(classification_report(y_test, y_pred))  precision recall f1-score support 1 0.68 0.62 0.65 439 2 0.91 0.88 0.89 3500 3 0.61 0.85 0.71 1268 4 0.86 0.66 0.75 2575 5 0.00 0.00 0.00 7 6 1.00 0.30 0.46 1623 7 0.30 0.68 0.42 1122 accuracy 0.70 10534 macro avg 0.62 0.57 0.55 10534 weighted avg 0.80 0.70 0.71 10534  fig, ax = plt.subplots(1, 1, figsize=(10, 10)) cm = confusion_matrix(y_test, y_pred) ax = plot_confusion_matrix(cm, class_names=class_lookup_l1[\u0026#34;lun_l1\u0026#34;], switch_axes=True, ax=ax) Let\u0026rsquo;s say we are happy with that result by now. Then we can create a map.\nCreate a map The data here is so small, that it would be also quite easy to perform the whole prediction by reading all the data, reshape to a 2-dimensional array or DataFrame, predict, reshape the predictions back to a 3-dimensional raster-like format, and write it to a raster file. But if the data gets larger and does not fit in memory more boilerplate code would be required to perform this process in chunks.\nHere we will use the EOCube class from the package eobox. The EOCube class allows passing any custom function on spatial chunks of single-band raster layers with the same extent and pixel alignment. It allows us to do the above-mentioned steps in spatial chunks (or windows) and process the data with any custom defined function. Thus, you can focus on developing the core functionality instead of boilerplate code. You can also find out more about the class in the tutorials An Intro to EOCube and Visualization with  EOCube in the eobox package documentation.\nLet us initialize an instance of EOCube with the data we need for prediction. Therefore, we need a DataFrame defining the layer stack which needs a column named uname with unique names and a column named path with the file paths of the rasters. We have created such a DataFrame above and can use it here:\nlandsat_layers.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  uname date band path     0 20190412_B1 2019-04-12 B1 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   1 20190412_B2 2019-04-12 B2 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   2 20190412_B3 2019-04-12 B3 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   3 20190412_B4 2019-04-12 B4 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...   4 20190412_B5 2019-04-12 B5 data_federsee/l8_aoi/LC08_L1TP_194026_20190412...     eoc = EOCube(landsat_layers, chunksize=2**6) We want to apply the following core function developed for a DataFrame to the whole raster. It is an extension of sklearn\u0026lsquo;s predict_proba method. It will return the probabilities, as returned by predict_proba, together with the target class ID (according to the maximum probability) and two confidence layers, the maximum probability and the difference between the maximum and the second-highest probability.\nWe define the in the next cell, however, the details are not important here. The important message is: You can define any custom function with the following properties:\n The first input argument is a DataFrame where the rows represent pixels and the columns represent single-band raster layers. It can have any additional arguments that are needed in the function. It returns a DataFrame with the same number of rows, still the pixels, and any number of columns that will later be written to the output raster layers.  # this function can also be imported from eobox as follows: # from eobox.ml import predict_extended def predict_extended(df, clf): \u0026#34;\u0026#34;\u0026#34;Derive probabilities, predictions, and condfidence layers. Parameters ---------- df : pandas.DataFrame DataFrame containing the data matris X to be predicted with `clf`. clf : sklearn.Classifier Trained sklearn classfifier with a `predict_proba` method. Returns ------- pandas.DataFrame DataFrame with the same number of rows as ``df`` and (n_classes + 3) columns. The columns contain the class predictions, confidence layers (max. probability and the difference between the max. and second highest probability), and class probabilities. \u0026#34;\u0026#34;\u0026#34; def convert_to_uint8(arr): return arr.astype(np.uint8) probs = clf.predict_proba(df.values) pred_idx = probs.argmax(axis=1) pred = np.zeros_like(pred_idx).astype(np.uint8) for i in range(probs.shape[1]): pred[pred_idx == i] = clf.classes_[i] # get reliability layers: # maximum probability # margin, i.e.maximum probability minus second highest probability probs_sorted = np.sort(probs, axis=1) max_prob = probs_sorted[:, probs_sorted.shape[1] - 1] margin = ( probs_sorted[:, probs_sorted.shape[1] - 1] \\ - probs_sorted[:, probs_sorted.shape[1] - 2] ) probs = convert_to_uint8(probs * 100) max_prob = convert_to_uint8(max_prob * 100) margin = convert_to_uint8(margin * 100) ndigits = len(str(max(clf.classes_))) prob_names = [f\u0026#34;prob_{cid:0{ndigits}d}\u0026#34; for cid in clf.classes_] df_result = pd.concat( [ pd.DataFrame({\u0026#34;pred\u0026#34;: pred, \u0026#34;max_prob\u0026#34;: max_prob, \u0026#34;margin\u0026#34;: margin}), pd.DataFrame(probs, columns=prob_names), ], axis=1, ) return df_result We can apply this function directly on the test set and get the desired outcomes.\npred_ext = predict_extended(testset[landsat_layers[\u0026#34;uname\u0026#34;]], rf_clf) pred_ext.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  pred max_prob margin prob_1 prob_2 prob_3 prob_4 prob_5 prob_6 prob_7     0 3 68 46 0 2 68 7 0 0 21   1 3 86 74 0 0 86 0 0 0 12   2 3 91 82 0 0 91 0 0 0 9   3 3 73 49 0 0 73 2 0 0 24   4 3 95 91 0 0 95 0 0 0 4     But how to apply this function, or any other custom function working on the DataFrame representation of a raster stack, on the raster EOCube data? We can wrap the function in a small function that we then can pass to the apply_and_write method of EOCube.\nFor development such a function it is useful to get one chunk of data as DataFrame, as it will also happen later in apply_and_write, and see if everything works.\nBefore we also need some destination file paths for storing the final results. Note that currently the chunks are stored as GeoTiffs and the full image layers as VRTs.\ndst_paths = [Path(\u0026#34;./data_federsee/map/\u0026#34;) \\ / (col + \u0026#34;.vrt\u0026#34;) for col in pred_ext] dst_paths [PosixPath('data_federsee/map/pred.vrt'), PosixPath('data_federsee/map/max_prob.vrt'), PosixPath('data_federsee/map/margin.vrt'), PosixPath('data_federsee/map/prob_1.vrt'), PosixPath('data_federsee/map/prob_2.vrt'), PosixPath('data_federsee/map/prob_3.vrt'), PosixPath('data_federsee/map/prob_4.vrt'), PosixPath('data_federsee/map/prob_5.vrt'), PosixPath('data_federsee/map/prob_6.vrt'), PosixPath('data_federsee/map/prob_7.vrt')]  ji = 1 eoc_chunk = eoc.get_chunk(ji) eoc_chunk = eoc_chunk.read_data() eoc_chunk = eoc_chunk.convert_data_to_dataframe() eoc_chunk_pred = predict_extended(eoc_chunk.data, rf_clf).astype(\u0026#34;uint8\u0026#34;) eoc_chunk.write_dataframe(result=eoc_chunk_pred, dst_paths=dst_paths) This produced the following files:\nlist(Path(\u0026#34;./data_federsee/map/\u0026#34;).rglob(\u0026#34;*.tif\u0026#34;)) [PosixPath('data_federsee/map/xchunks_cs64/prob_4/prob_4_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/prob_5/prob_5_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/prob_1/prob_1_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/pred/pred_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/max_prob/max_prob_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/prob_2/prob_2_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/prob_3/prob_3_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/prob_7/prob_7_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/margin/margin_ji-01.tif'), PosixPath('data_federsee/map/xchunks_cs64/prob_6/prob_6_ji-01.tif')]  Then, if everything works as desired we can wrap these lines in a function as follows and process all chunks with it.\ndef fun(eoc_chunk, dst_paths, clf): eoc_chunk = eoc_chunk.read_data().convert_data_to_dataframe() pred = predict_extended(eoc_chunk.data, rf_clf).astype(\u0026#34;uint8\u0026#34;) eoc_chunk.write_dataframe(result=pred, dst_paths=dst_paths) return eoc_chunk.ji eoc.apply_and_write(fun=fun, dst_paths=dst_paths, clf=rf_clf)  6%|▌ | 1/17 [00:00\u0026lt;00:03, 5.23it/s] 1 chunks already processed and skipped. 100%|██████████| 17/17 [00:03\u0026lt;00:00, 5.13it/s]  As a result, we get all the output layers for the whole image as VRTs:\nlist(Path(\u0026#34;./data_federsee/map/\u0026#34;).rglob(\u0026#34;*.vrt\u0026#34;)) [PosixPath('data_federsee/map/prob_7.vrt'), PosixPath('data_federsee/map/prob_1.vrt'), PosixPath('data_federsee/map/pred.vrt'), PosixPath('data_federsee/map/prob_3.vrt'), PosixPath('data_federsee/map/prob_2.vrt'), PosixPath('data_federsee/map/margin.vrt'), PosixPath('data_federsee/map/prob_6.vrt'), PosixPath('data_federsee/map/max_prob.vrt'), PosixPath('data_federsee/map/prob_4.vrt'), PosixPath('data_federsee/map/prob_5.vrt')]  These can usually be used as any other raster format.\nTo finalize this section, let us have a look at the prediction and one of the confidence layers.\nfig, ax = plt.subplots(2, 1, figsize=(16, 6)) with rasterio.open(\u0026#34;./data_federsee/map/pred.vrt\u0026#34;) as src: arr = src.read() sns.heatmap(arr[0,: , :], cmap=sns.color_palette(class_lookup_l1[\u0026#34;color\u0026#34;]), square=True, xticklabels=False, yticklabels=False, ax=ax[0], ) with rasterio.open(\u0026#34;./data_federsee/map/margin.vrt\u0026#34;) as src: arr = src.read() sns.heatmap(arr[0,: , :], cmap=\u0026#34;RdBu\u0026#34;, square=True, xticklabels=False, yticklabels=False, ax=ax[1], ) The End In this post, we walked through the process of classifying remote sensing images. We used the Python package eobox to apply the machine learning capabilities of sklearn to geospatial raster data.\nI am happy if this post or the eobox package is helpful for anybody. As always, I am also happy about any critical feedback from which I can learn.\n",
    "ref": "/blog/2020-01-06-1_federsee-blog-series_part-3_clf/"
  },{
    "title": "Satellite imagery classification - II",
    "date": "",
    "description": "Downloading and preparing OpenStreetMap data",
    "body": "Context and Content This is the second part of a three parts series about using remote sensing data to classify the earth\u0026rsquo;s surface. Please read the first part of the series if you are interested in an introduction to the post series and / or in the first part where we downloaded parts of Landsat scenes by leveraging the Cloud Optimized GeoTIFF format.\nIn this part, after a short introduction to OpenStreetMap (OSM), we will download and prepare land use and land cover data sourced from OSM. The data will be used to create labeled data for the supervised classification task in the next post.\nWhat is OSM? Let us start with two quotes from the OpenStreetMap Wiki - About OpenStreetMap:\nOpenStreetMap is a free, editable map of the whole world that is being built by volunteers largely from scratch and released with an open-content license.\nThe OpenStreetMap License allows free (or almost free) access to our map images and all of our underlying map data. The project aims to promote new and interesting uses of this data. See \u0026ldquo;Why OpenStreetMap?\u0026quot; for more details about why we want an open-content map and for the answer to the question we hear most frequently: Why not just use Google maps?\nOSM data model According to the OSM Wiki Features are mappable physical landscape elements. Elements allow us to define where these features are in space and which of and how the elements belong together. Tags tell us more about the elements.\nElements The OSM database is a collection of the following elements\nnodes : points in space defined by latitude and longitude coordinates ways : linear features and area boundaries defined by multiple nodes relations: sometimes used to explain how other elements work together  Tags Tags are described in the Wiki as follows:\nA tag consists of two items, a key and a value. Tags describe specific features of map elements (nodes, ways, or relations) [\u0026hellip;]. OpenStreetMap Wiki - Tags\nThe OpenStreetMap Wiki - Map Features page lists such key value pairs. In this list we can find - among many others - the key natural and landuse. Later we will download the data for our AOI which has been been tagged with one of these two keys. We use the tags\u0026rsquo; values as labels for the land use and land cover classification in the next post. Note that also other keys contain useful information for a land use and land cover classification but for this post series we only select two to keep it simple.\nExample Federsee Let us see how the abstract description above looks like in practice.\nThe following screenshot shows OSM representation of the Federsee on www.openstreetmap.org. The Federsee (8387767) is a relation which consists of two ways (see Members), an outer and an inner one since there is an island in the lake. If we click on the IDs of the ways in the lower left corner we would see that each of them is made up by several nodes. These elements allow to draw the feature on the map. Besides that, additional information about the feature is available via the tags, e.g. the name is Federsee, value for the key natural is water, etc. Note that the tags are also used for selecting the style in which to draw a feature.\nDownload options for OpenStreetMap data There are several ways how you can get the OSM data. One possibility is to navigate to the AOI in the Export Tab of the OpenStreetMap Webpage. From there you can download the data of an area of interest (AOI) as a OSM XML file.\nYou can also use the Overpass API which is a read-only API [\u0026hellip;] optimized for data consumers that need a few elements within a glimpse or up to roughly 10 million elements. A nice post for getting started is Loading Data from OpenStreetMap with Python and the Overpass API which also references Overpass Torbo. The latter is is practical to evaluate queries in the browser. The whole data for the AOI can be queried as follows:\n/* This is a simple map call. It returns all data in the bounding box. */ [out:xml]; ( node(48.0680, 9.5430, 48.1068, 9.6744); \u0026lt;; ); out meta;  overpy and overpass are two Python APIs for accessing the Overpass API. On October 3rd, 2019 overpy had 110 stars 41 forks and overpass 186 stars, 66 forks.\nIn this post we use yet another option, i.e. the osmnx package. This package is very nice because it allows us to download the data of our AOI with a landuse or natural key and convert it in a geopandas.GeoDataFrame in one line of code.\nSince we now know better what OSM data is, which part of the data we want to download and which framework we want to use, we can get start with the hands-on part of this post.\nDownloading and preparing OSM data with osmnx and pandas Libraries Following libraries will be used in this post:\nimport geopandas as gpd from matplotlib.patches import Patch import numpy as np import osmnx as ox import pandas as pd Area of interest We load the AOI from the GeoJSON ./data_federsee/aoi_federsee_4326.geojson which we stored in the [first part of the series]({% post_url 2019-09-29-1_federsee-blog-series_part-1_cog %}). The content of the file looks as follows:\n%%bash cat ./data_federsee/aoi_federsee_4326.geojson { \u0026quot;type\u0026quot;: \u0026quot;FeatureCollection\u0026quot;, \u0026quot;crs\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;name\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;urn:ogc:def:crs:OGC:1.3:CRS84\u0026quot; } }, \u0026quot;features\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;Feature\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;ID\u0026quot;: 1, \u0026quot;Name\u0026quot;: \u0026quot;Federsee\u0026quot; }, \u0026quot;geometry\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;Polygon\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ [ 9.543, 48.1068 ], [ 9.6744, 48.1068 ], [ 9.6744, 48.068 ], [ 9.543, 48.068 ], [ 9.543, 48.1068 ] ] ] } } ] }  gdf_aoi_4326 = gpd.read_file( \u0026#34;./data_federsee/aoi_federsee_4326.geojson\u0026#34; ) gdf_aoi_4326  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  ID Name geometry     0 1 Federsee POLYGON ((9.54300 48.10680, 9.67440 48.10680, ...     Download OSM data with osmnx With the osmnx function create_footprints_gdf it does not take more than a line of code to download the data for a specific AOI and footprint type (or OSM tag key) such as \u0026lsquo;building\u0026rsquo;, \u0026lsquo;landuse\u0026rsquo;, \u0026lsquo;place\u0026rsquo;, etc. and convert the data into a geopandas.GeoDataFrame.\nSince we are interested in land use and land cover information we download the OSM data of the footprint types \u0026lsquo;natural\u0026rsquo; and \u0026lsquo;landuse\u0026rsquo;.\nDownload and prepare \u0026lsquo;natural\u0026rsquo; Let us download the data and see what we get.\ngdf_natural = ox.create_footprints_gdf( polygon=gdf_aoi_4326.loc[0, \u0026#34;geometry\u0026#34;], footprint_type=\u0026#34;natural\u0026#34;, retain_invalid=True, ) print( \u0026#34;Number of (rows, columns) of the Geodataframe:\u0026#34;, gdf_natural.shape, ) print(\u0026#34;\\nFirst two rows of the GeoDataFrame:\u0026#34;) display(gdf_natural.head(2)) print( \u0026#34;\\nTransposed description of the GeoDataFrame without geometry, nodes:\u0026#34; ) # describe() does not work on the three columns we drop here first gdf_natural.drop( [\u0026#34;geometry\u0026#34;, \u0026#34;nodes\u0026#34;, \u0026#34;members\u0026#34;], axis=1 ).describe().transpose() Number of (rows, columns) of the Geodataframe: (102, 22) First two rows of the GeoDataFrame:   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  nodes natural geometry water wetland leaf_cycle leaf_type description intermittent man_made ... operator species members type TMC:cid_58:tabcd_1:Class TMC:cid_58:tabcd_1:LCLversion TMC:cid_58:tabcd_1:LocationCode name wikidata wikipedia     31829954 [356416115, 3959793379, 356416144, 2486677463,... water POLYGON ((9.54352 48.10559, 9.54360 48.10561, ... NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   31829956 [356416148, 2486677471, 2486677470, 356416186,... water POLYGON ((9.54280 48.10632, 9.54273 48.10634, ... NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN    2 rows × 22 columns\n Transposed description of the GeoDataFrame without geometry, nodes:   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  count unique top freq     natural 102 6 wetland 27   water 13 2 pond 12   wetland 19 4 wet_meadow 8   leaf_cycle 12 1 deciduous 12   leaf_type 14 1 broadleaved 14   description 1 1 Regenüberlaufbecken 1   intermittent 1 1 yes 1   man_made 1 1 basin 1   attraction 1 1 animal 1   barrier 1 1 fence 1   operator 1 1 Metallbau Knoll 1   species 1 1 chicken, goat, sheep 1   type 4 1 multipolygon 4   TMC:cid_58:tabcd_1:Class 1 1 Area 1   TMC:cid_58:tabcd_1:LCLversion 1 1 9.00 1   TMC:cid_58:tabcd_1:LocationCode 1 1 42084 1   name 2 2 Federseeried 1   wikidata 1 1 Q248234 1   wikipedia 1 1 de:Federsee 1     We got a GeoDataFrame with 102 rows and 22 columns. Each row is a feature of the OSM database and is identifiable by the OSM ID stored in the index. It contains the elements data of OSM in the nodes, geometry and members columns.\nOther columns represent the tag keys (column names) and values (values in the cells of the GeoDataFrame). The column natural is completely filled since we downloaded the footprint type \u0026lsquo;natural\u0026rsquo;. All the other tag columns contain various degrees of missing values as can be seen from the described() output.\nRemember that our goal is to prepare the data for a land use and land cover classification with 30m resolution imagery (Part 3 of this post series). Therefore it makes sense to only keep \u0026lsquo;Polygon\u0026rsquo; and \u0026lsquo;MultiPolygon\u0026rsquo; geometries.\nrows_keep_natural = gdf_natural.geometry.type.isin( [\u0026#34;Polygon\u0026#34;, \u0026#34;MultiPolygon\u0026#34;] ) print(\u0026#34;Counts of removed tag values.\u0026#34;) display( pd.crosstab( gdf_natural.loc[~rows_keep_natural, :].type, gdf_natural.loc[~rows_keep_natural, \u0026#34;natural\u0026#34;], ) ) gdf_natural = gdf_natural.loc[rows_keep_natural, :] Counts of removed tag values.   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n natural tree_row   row_0      LineString 17     We also want to keep only the columns / tags which might contain interesting information with regard to classification task.\nkeep_tag_columns_natural = [ \u0026#34;natural\u0026#34;, \u0026#34;water\u0026#34;, \u0026#34;wetland\u0026#34;, \u0026#34;leaf_cycle\u0026#34;, \u0026#34;leaf_type\u0026#34;, ] for col in keep_tag_columns_natural[1::]: display(pd.crosstab(gdf_natural[\u0026#34;natural\u0026#34;], gdf_natural[col]))  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n water pond reservoir   natural       water 12 1      .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n wetland marsh reedbed swamp wet_meadow   natural         wetland 2 6 3 8      .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n         .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n leaf_type broadleaved   natural      wood 1     As we can see leaf_cycle column is empty so we remove it from the initial list. This is probably because there were only values in the removed rows.\n# as we can see the leave keep_tag_columns_natural = [ \u0026#34;natural\u0026#34;, \u0026#34;water\u0026#34;, \u0026#34;wetland\u0026#34;, \u0026#34;leaf_type\u0026#34;, ] gdf_natural = gdf_natural.loc[ :, [\u0026#34;geometry\u0026#34;] + keep_tag_columns_natural ] Download and prepare \u0026lsquo;landuse\u0026rsquo; We do the same as above\u0026hellip;\ngdf_landuse = ox.create_footprints_gdf( polygon=gdf_aoi_4326.loc[0, \u0026#34;geometry\u0026#34;], footprint_type=\u0026#34;landuse\u0026#34;, retain_invalid=True, ) rows_keep_landuse = gdf_landuse.geometry.type.isin( [\u0026#34;Polygon\u0026#34;, \u0026#34;MultiPolygon\u0026#34;] ) print(\u0026#34;Counts of removed tag values.\u0026#34;) display( pd.crosstab( gdf_landuse.loc[~rows_keep_landuse, :].type, gdf_landuse.loc[~rows_keep_landuse, \u0026#34;landuse\u0026#34;], ) ) gdf_landuse = gdf_landuse.loc[rows_keep_landuse, :] print( \u0026#34;\\nNumber of (rows, columns) of the Geodataframe:\u0026#34;, gdf_natural.shape, ) print(\u0026#34;\\nFirst two rows of the GeoDataFrame:\u0026#34;) display(gdf_landuse.head(2)) print( \u0026#34;\\nTransposed GeoDataFrame description without geometry, nodes:\u0026#34; ) # describe() does not work on the three columns we drop here first gdf_landuse.drop( [\u0026#34;geometry\u0026#34;, \u0026#34;nodes\u0026#34;, \u0026#34;members\u0026#34;], axis=1 ).describe().transpose() keep_tag_columns_landuse = [\u0026#34;landuse\u0026#34;, \u0026#34;leaf_type\u0026#34;] for col in keep_tag_columns_landuse[1::]: display(pd.crosstab(gdf_landuse[\u0026#34;landuse\u0026#34;], gdf_landuse[col])) # keep what we need for the classification gdf_landuse = gdf_landuse.loc[ :, [\u0026#34;geometry\u0026#34;] + keep_tag_columns_landuse ] Counts of removed tag values.   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n        Number of (rows, columns) of the Geodataframe: (85, 5) First two rows of the GeoDataFrame:   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  nodes landuse geometry name toilets:wheelchair tourism wheelchair source leaf_type religion basin members type     8030302 [60020682, 1369161162, 1369161159, 60020683, 1... forest POLYGON ((9.67498 48.09177, 9.67556 48.09173, ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   8030303 [60020695, 4878237619, 4878237618, 1369161194,... forest POLYGON ((9.67105 48.09603, 9.67099 48.09655, ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN     Transposed GeoDataFrame description without geometry, nodes:   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n leaf_type broadleaved mixed needleleaved   landuse        forest 2 1 2     Remove spatial overlaps Before we combine the data we want to check if there are overlaps between\n the (multi)-polygons of each GeoDataFrame between the (multi)-polygons of the two GeoDataFrames.  For this tutorial we keep it simple and remove these (multi-)polygons as suspicious or ambiguous cases. But of course in a real world scenario it might be worth looking into that cases in more detail to find out if these polygons are useful.\nTo get overlaps between (multi-)polygons in one Geodataframe we define the following function. It takes a dataframe and returns None if there are no overlapping polygons and the subset of overlapping polygons otherwise.\ndef get_overlapping_polygons(gdf): \u0026#34;\u0026#34;\u0026#34;Get the overlap between polygons in the same dataframe.\u0026#34;\u0026#34;\u0026#34; # This is not a performant solution! overlaps = [] for i, row_i in gdf.iterrows(): for j, row_j in gdf.iterrows(): if i \u0026lt; j: if row_i.geometry.overlaps(row_j.geometry): overlaps.append(i) overlaps.append(j) if overlaps: return gdf.loc[np.unique(overlaps), :] else: return None gdf_natural_overlaps = get_overlapping_polygons(gdf_natural) gdf_natural_overlaps There are no overlaping polygons in gdf_natural.\ngdf_landuse_overlaps = get_overlapping_polygons(gdf_landuse) gdf_landuse_overlaps  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  geometry landuse leaf_type     5747104 POLYGON ((9.57793 48.06533, 9.57764 48.06582, ... forest NaN   5747254 POLYGON ((9.59307 48.06887, 9.59314 48.06881, ... meadow NaN   6043203 POLYGON ((9.72595 48.11495, 9.72592 48.11505, ... forest NaN   9945268 POLYGON ((9.65546 48.09192, 9.65490 48.09161, ... residential NaN   601113438 POLYGON ((9.65429 48.08923, 9.65427 48.08917, ... farmyard NaN   601113454 POLYGON ((9.66902 48.10880, 9.67136 48.10888, ... farmland NaN     There are 6 overlaping polygons in gdf_landuse.\nLet us inspect what we loose when we just remove these polygons spatially and in terms of polygon counts per class.\nax = gdf_aoi_4326.plot(color=\u0026#34;#d95f02\u0026#34;, figsize=(16, 10)) ax = gdf_landuse_overlaps.plot(color=\u0026#34;#7570b3\u0026#34;, ax=ax) txt = ax.set_title( \u0026#34;AOI (orange) and overlapping landuse polygons (violet).\u0026#34; ) For the polygon counts comparison we define a function since we will need the same later.\ndef compare_value_counts(df1, df2, col): \u0026#34;\u0026#34;\u0026#34;Count the values of the same column (col) and create a dataframe with counts and count differences.\u0026#34;\u0026#34;\u0026#34; value_counts_1 = df1[col].value_counts() value_counts_2 = df2[col].value_counts() df_comparison = pd.concat( [ value_counts_1.rename(f\u0026#34;{col}_1\u0026#34;), value_counts_2.rename(f\u0026#34;{col}_2\u0026#34;), (value_counts_1 - value_counts_2).rename(\u0026#34;1 - 2\u0026#34;), ], axis=1, sort=False, ) return df_comparison compare_value_counts( gdf_landuse, gdf_landuse.drop(gdf_landuse_overlaps.index), col=\u0026#34;landuse\u0026#34;, )  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  landuse_1 landuse_2 1 - 2     meadow 71 70 1   farmland 48 47 1   forest 21 19 2   farmyard 14 13 1   residential 13 12 1   orchard 7 7 0   allotments 6 6 0   commercial 5 5 0   plant_nursery 3 3 0   industrial 3 3 0   cemetery 2 2 0   grass 2 2 0   basin 1 1 0     Let us decide that we can get over it when we remove these polygons and do it.\ngdf_landuse = gdf_landuse.drop(gdf_landuse_overlaps.index) To get overlaping (multi-)polygons between two Geodataframe we can use the overlay functionality of geopandas.\ngdf_intersection = gpd.overlay( gdf_landuse.reset_index(), gdf_natural.reset_index(), how=\u0026#34;intersection\u0026#34;, ) display(gdf_intersection) ax = gdf_aoi_4326.plot(color=\u0026#34;#d95f02\u0026#34;, figsize=(16, 10)) # gdf_intersection.plot(color=\u0026#34;#7570b3\u0026#34;, ax=ax) ax = gdf_landuse.loc[gdf_intersection[\u0026#34;index_1\u0026#34;]].plot( color=\u0026#34;#7570b3\u0026#34;, ax=ax ) ax = gdf_natural.loc[gdf_intersection[\u0026#34;index_2\u0026#34;]].plot( color=\u0026#34;#1b9e77\u0026#34;, ax=ax ) txt = ax.set_title( \u0026#34;AOI (orange), overlapping landuse (violet) and natural (green)\u0026#34; \u0026#34;polygons.\u0026#34; )  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index_1 landuse leaf_type_1 index_2 natural water wetland leaf_type_2 geometry     0 129014138 commercial NaN 216506723 water pond NaN NaN POLYGON ((9.64559 48.06889, 9.64563 48.06883, ...   1 129014140 residential NaN 600407995 water pond NaN NaN POLYGON ((9.64581 48.07079, 9.64586 48.07075, ...   2 601767693 meadow NaN 601767688 grassland NaN NaN NaN POLYGON ((9.66622 48.07509, 9.66598 48.07507, ...   3 715419648 allotments NaN 715419645 wood NaN NaN NaN POLYGON ((9.62574 48.10542, 9.62576 48.10541, ...   4 715706685 meadow NaN 715706686 wetland NaN NaN NaN POLYGON ((9.59961 48.09108, 9.60090 48.09078, ...   5 715706685 meadow NaN 715706687 wetland NaN NaN NaN POLYGON ((9.59995 48.09326, 9.60021 48.09306, ...   6 7283742 meadow NaN 496089851 scrub NaN NaN NaN POLYGON ((9.66512 48.09539, 9.66490 48.09536, ...     compare_value_counts( gdf_landuse, gdf_landuse.drop(gdf_intersection[\u0026#34;index_1\u0026#34;]), col=\u0026#34;landuse\u0026#34;, )  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  landuse_1 landuse_2 1 - 2     meadow 70 67 3   farmland 47 47 0   forest 19 19 0   farmyard 13 13 0   residential 12 11 1   orchard 7 7 0   allotments 6 5 1   commercial 5 4 1   plant_nursery 3 3 0   industrial 3 3 0   cemetery 2 2 0   grass 2 2 0   basin 1 1 0     compare_value_counts( gdf_natural, gdf_natural.drop(gdf_intersection[\u0026#34;index_2\u0026#34;]), col=\u0026#34;natural\u0026#34;, )  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  natural_1 natural_2 1 - 2     wetland 27 25 2   water 22 20 2   wood 19 18 1   grassland 11 10 1   scrub 6 5 1     Let us also decide that we can get over that lost.\ngdf_landuse = gdf_landuse.drop(gdf_intersection[\u0026#34;index_1\u0026#34;]) gdf_natural = gdf_natural.drop(gdf_intersection[\u0026#34;index_2\u0026#34;]) Combine \u0026lsquo;landuse\u0026rsquo; and \u0026lsquo;natural\u0026rsquo; We combine the two GeoDataFrames with a simple non-spatial concatenate operation. We are confident that we do not get any overlapping polygons since we removed them already. Still we add an assertion statement here to make sure that this is true.\nAlso we make the assertion that there we do not have a value for \u0026lsquo;landuse\u0026rsquo; and \u0026lsquo;natural\u0026rsquo; in any row since we want to combine this information in one column.\ngdf_lun = pd.concat([gdf_landuse, gdf_natural], sort=False) assert get_overlapping_polygons(gdf_lun) is None assert (gdf_lun[[\u0026#34;landuse\u0026#34;, \u0026#34;natural\u0026#34;]].isna().sum(axis=1) == 1).all() Since we did not get an AssertionError our assertions are valid and we can create the label column for the classification task. Let us call it \u0026lsquo;lun\u0026rsquo; for \u0026lsquo;landuse\u0026rsquo; and \u0026lsquo;natural\u0026rsquo;.\ngdf_lun[\u0026#34;lun\u0026#34;] = gdf_lun[\u0026#34;landuse\u0026#34;] gdf_lun.loc[gdf_lun[\u0026#34;landuse\u0026#34;].isna(), \u0026#34;lun\u0026#34;] = gdf_lun[\u0026#34;natural\u0026#34;] Final cleansing In a final step we\n merge \u0026lsquo;commercial\u0026rsquo;, \u0026lsquo;industrial\u0026rsquo;, \u0026lsquo;residential\u0026rsquo;, \u0026lsquo;farmyard\u0026rsquo; into a \u0026lsquo;buildup\u0026rsquo; class (knowning that they might include vegetated areas), merge \u0026lsquo;meadow\u0026rsquo;, \u0026lsquo;grassland\u0026rsquo; into one \u0026lsquo;grassland\u0026rsquo; class, merge \u0026lsquo;forest\u0026rsquo;, \u0026lsquo;wood\u0026rsquo; into one \u0026lsquo;forest\u0026rsquo; class (knowing that we stretch the meanding of forest here), remove polygons of very small size, i.e. smaller than 5 pixels, remove label categories with less than 3 polygons (after the steps above).  gdf_lun[\u0026#34;lun_l1\u0026#34;] = gdf_lun[\u0026#34;lun\u0026#34;] gdf_lun.loc[ gdf_lun[\u0026#34;lun_l1\u0026#34;].isin( [\u0026#34;commercial\u0026#34;, \u0026#34;industrial\u0026#34;, \u0026#34;residential\u0026#34;, \u0026#34;farmyard\u0026#34;] ), \u0026#34;lun_l1\u0026#34;, ] = \u0026#34;buildup\u0026#34; gdf_lun.loc[ gdf_lun[\u0026#34;lun_l1\u0026#34;].isin([\u0026#34;meadow\u0026#34;, \u0026#34;grassland\u0026#34;]), \u0026#34;lun_l1\u0026#34; ] = \u0026#34;grassland\u0026#34; gdf_lun.loc[ gdf_lun[\u0026#34;lun_l1\u0026#34;].isin([\u0026#34;forest\u0026#34;, \u0026#34;wood\u0026#34;]), \u0026#34;lun_l1\u0026#34; ] = \u0026#34;forest\u0026#34; gdf_lun[\u0026#34;area_m2\u0026#34;] = gdf_lun.to_crs(crs={\u0026#34;init\u0026#34;: \u0026#34;epsg:32632\u0026#34;}).area gdf_lun = gdf_lun[gdf_lun[\u0026#34;area_m2\u0026#34;] \u0026gt; 5 * 30 * 30] n_polygons_per_class = gdf_lun[\u0026#34;lun_l1\u0026#34;].value_counts() gdf_lun = gdf_lun[ gdf_lun[\u0026#34;lun_l1\u0026#34;].isin( n_polygons_per_class[n_polygons_per_class \u0026gt;= 3].index ) ] Visualize and store the cleansed data Usually we want to use a specific color map for land use / land cover maps. With geopandas.GeoDataFrame.plot we cannot specify such custom color maps. Therefore we need create a column containing the colors and create a legend by making use of the lower-level matplotlib plotting functions.\nHere is what our labeled data for the classification task in the next post of this series looks like.\n# define color code for lun_l1 and add as colors as column class_color_map = { \u0026#34;buildup\u0026#34;: \u0026#34;#e31a1c\u0026#34;, \u0026#34;farmland\u0026#34;: \u0026#34;#ff7f00\u0026#34;, \u0026#34;forest\u0026#34;: \u0026#34;#33a02c\u0026#34;, \u0026#34;grassland\u0026#34;: \u0026#34;#b2df8a\u0026#34;, \u0026#34;orchard\u0026#34;: \u0026#34;#b15928\u0026#34;, \u0026#34;water\u0026#34;: \u0026#34;#1f78b4\u0026#34;, \u0026#34;wetland\u0026#34;: \u0026#34;#a6cee3\u0026#34;, } gdf_lun[\u0026#34;color\u0026#34;] = gdf_lun[\u0026#34;lun_l1\u0026#34;].map(class_color_map) # create legend elements  n_polygons_per_class = gdf_lun[\u0026#34;lun_l1\u0026#34;].value_counts() legend_elements = [ Patch( facecolor=value, edgecolor=\u0026#34;black\u0026#34;, label=f\u0026#34;{key} ({n_polygons_per_class[key]})\u0026#34;, ) for key, value in class_color_map.items() ] # plot the aoi and the lun_l1 column  ax = gdf_aoi_4326.plot( color=\u0026#34;black\u0026#34;, edgecolor=\u0026#34;black\u0026#34;, figsize=(16, 10) ) txt = ax.set_title(\u0026#34;Land use and land cover classes.\u0026#34;) ax = gdf_lun.plot( color=gdf_lun[\u0026#34;color\u0026#34;], edgecolor=\u0026#34;black\u0026#34;, ax=ax, ) # add legend to plot ax = ax.legend( handles=legend_elements, title=\u0026#34;Class (Polygons Count)\u0026#34;, loc=\u0026#34;upper right\u0026#34;, ncol=2, ) Let us store the data such that it is available in the next post.\ngdf_lun.to_file( \u0026#34;./data_federsee/osm_feedersee_cleansed_4326.geojson\u0026#34;, driver=\u0026#34;GeoJSON\u0026#34;, ) The end OpenStreetMap is a great open project. The number of people involved and data collected is impressive.\nAccording to the OpenStreetMap stats report run at 2019-10-24 22:00:06 +0000 the numbers look as follows:\nNumber of users 5,769,940 Number of uploaded GPS points 7,547,182,003 Number of nodes 5,549,201,409 Number of ways 615,216,915 Number of relations 7,201,811 In this post we downloaded a small part of the OSM data with osmnx and manipulated the data with geopandas and pandas. Together with the satellite data downloaded in the [first part of the series]({% post_url 2019-09-29-1_federsee-blog-series_part-1_cog %}) we do now have the two incredients that we need for creating a land use and land cover map with a supervised classification algorithm in the next and final post of this post series.\nPlease feel free to leave a comment or contact me if you have any feedback.\n",
    "ref": "/blog/2019-10-26-1_federsee-blog-series_part-2_osm/"
  },{
    "title": "Satellite imagery classification - I",
    "date": "",
    "description": "Leveraging Cloud Optimized GeoTIFFs to download parts of Landsat scenes",
    "body": "Introduction to the post series Outline This is a three parts series about classification of remote sensing images. Remote sensing images are already beautiful enough to only look at, but they can also be used for mapping the earth\u0026rsquo;s surface. When the task is to map categorical classes, such as forest, water, meadow, farmland, residential area, etc. the task can be solved by classification, often more specifically called land use and/or land cover classification.\nIn this series we will follow the supervised machine learing paradigm to perform the classification. For that, we need two main ingredients which we will download and prepare in the first two parts of the post series:\nIn Part 1 of this series we will download the first incredient, i.e. the remote sensing images which will serve as features, or X. Particularly we will download Landsat scenes and leverage the Cloud Optimized GeoTiff format to only download a subset of the imagery.\nIn Part 2 we will download and prepare the second ingredient, i.e. georeferenced land use and land cover data which will serve as labels, or y. Particularly we will use OpenStreetMap data.\nIn Part 3 we will start cooking and combine the two datasets from the previous parts to train evaluate and apply a model. The model is able to predict the class label given the features, i.e. a list or image pixel values. We will also map the full area of interest (AOI) by applying the model to the full downloaded image.\nTo solve all these tasks we will use Python.\nIn Part 3 I will show how the eobox package can be used for the above mentioned tasks to be solved there. I have been working on this package for fun and learning in the end of 2018 but it grew to something that I believe can be helpeful for others.\nLet us start with Part 1. But before we dive into it let us import the libraries we need along the way and define the AOI.\nLibraries import folium import geopandas as gpd import matplotlib.pyplot as plt import numpy as np import pandas as pd from pathlib import Path from pyproj import Proj, transform import rasterio from shapely import geometry import subprocess from tqdm import tqdm Area of interest A remote sensing image classification project focuses on a special AOI, i.e. the area in which we want to gain new information or knowledge. Here AOI as around the Feedersee (i.e. feather lake in english) in the heart of Upper Swabia and defined by the bounding box coordinates (as latitudes and longitudes):\nnorth=48.1068 east=9.6744 south=48.0680 west=9.5430 Let us create a geopandas GeoDataFrame from the coordinates and save the AOI as a GeoPackage.\naoi_polygon = geometry.Polygon([[west, north], [east, north], [east, south], [west, south]]) gdf_aoi = gpd.GeoDataFrame(pd.DataFrame(data=[[1, \u0026#34;Federsee\u0026#34;]], columns=[\u0026#34;ID\u0026#34;, \u0026#34;Name\u0026#34;]), geometry=[aoi_polygon], crs={\u0026#39;init\u0026#39;: \u0026#39;epsg:4326\u0026#39;}) # safe as GeoPackage Path(\u0026#34;./data_federsee\u0026#34;).mkdir(exist_ok=True, parents=True) gdf_aoi.to_file(filename=\u0026#39;./data_federsee/aoi_federsee_4326.geojson\u0026#39;, driver=\u0026#39;GeoJSON\u0026#39;) Finally let\u0026rsquo;s see where we are and visualize the AOI with a OSM background layer.\nmap_osm = folium.Map() map_osm.fit_bounds([[south, west], [north, east]]) folium.GeoJson(gdf_aoi).add_to(map_osm) map_osm  Part 1: Leveraging Cloud Optimized GeoTIFFs to download parts of Landsat scenes Why Cloud Optimized GeoTIFF? In many, if not most, cases satellite imagery is distributed in quite hugh data packages. This is at least true for the past and present. For example, when you download a Landsat 8 Level-1 GeoTIFF Data Product from USGS EarthExplorer it will have around 900 MB. What if you need only a small part of that imagery such as here where we want to download only a small chunk of some bands of the whole scene? You need to download the whole cake even though you only want a piece or some crumbs of it.\nThe Cloud Optimized GeoTIFF (COG) format is on the way to chang that. Given that the data provider makes the data available as COGs it allow the clients [\u0026hellip;] to ask for just the parts of a file they need (quote from Cloud Optimized GeoTIFF).\nData in the Landsat PDS bucket is stored as COGs and this is why we download the data from there.\nIn Python rasterio is a great package to access to geospatial raster data.\nrasterio knows how to handle GOCs since it builds upon GDAL and GDAL handles COGs. Sean Gilles has written a great Jupyter notebook with Advanced features in Rasterio from which I took most of the code below. The notebook demonstrates five advanced features that are useful for developing cloud-native applications. We will make use of those helping us to download only the part of the Landsat imagery covering our AOI.\nWhat to download? If you are interested in only a few specific Landsat scenes it is convenient to browse and select for the imagery in the EO-Browser of Synergise. A part of the nice search and visualization features it also offers the AWS path of the scenes.\nIn this post we are interested in the following Landsat scenes:\n  http://landsat-pds.s3.amazonaws.com/c1/L8/194/026/LC08_L1TP_194026_20190903_20190903_01_RT/index.html\n  http://landsat-pds.s3.amazonaws.com/c1/L8/194/026/LC08_L1TP_194026_20190818_20190818_01_RT/index.html\n  http://landsat-pds.s3.amazonaws.com/c1/L8/194/026/LC08_L1TP_194026_20190412_20190412_01_RT/index.html\n  We want to download the bands 2, 3, 4, 5, 6 and 7 of these scenes but only the parts that overlap with the AOI.\nLet us first define variables containing with the scenes and bands we want to download.\nscenes = [\u0026#34;LC08_L1TP_194026_20190412_20190412_01_RT\u0026#34;, \u0026#34;LC08_L1TP_194026_20190818_20190818_01_RT\u0026#34;, \u0026#34;LC08_L1TP_194026_20190903_20190903_01_RT\u0026#34; ] bands = [f\u0026#34;B{band}\u0026#34; for band in range(1, 8)] And a function with which we can easily create the paths to the specific file. From the data in the Landsat PDS bucket we know that it is organized by path, row, and scene. So \u0026hellip;\ndef parse_aws_landsat_path(scene, band, ext=\u0026#34;TIF\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Parse a scene file path in the aws landsat bucket. \u0026#34;\u0026#34;\u0026#34; path = scene.split(\u0026#34;_\u0026#34;)[2][:3] row = scene.split(\u0026#34;_\u0026#34;)[2][3:] path = f\u0026#34;s3://landsat-pds/c1/L8/\u0026#34; \\ f\u0026#34;{path}/{row}/{scene}/{scene}_{band}.{ext}\u0026#34; return path # example fp = parse_aws_landsat_path(\u0026#34;LC08_L1TP_194026_20190818_20190818_01_RT\u0026#34;, \u0026#34;B5\u0026#34;, \u0026#34;TIF\u0026#34;) fp 's3://landsat-pds/c1/L8/194/026/LC08_L1TP_194026_20190818_20190818_01_RT/LC08_L1TP_194026_20190818_20190818_01_RT_B5.TIF'  We can open this dataset with rasterio as any GDAL readable raster file we would have stored locally. But before we need to set AWS credentials in our environment. Even if they are empty. But without we get an CPLE_AWSInvalidCredentialsError.\n%env AWS_SECRET_ACCESS_KEY=\u0026#34;\u0026#34; %env AWS_NO_SIGN_REQUEST=\u0026#34;\u0026#34; env: AWS_SECRET_ACCESS_KEY=\u0026quot;\u0026quot; env: AWS_NO_SIGN_REQUEST=\u0026quot;\u0026quot;  Now we can get access the data and, for example, get the metadata attribute of it.\nwith rasterio.open(fp) as src: meta = src.meta meta {'driver': 'GTiff', 'dtype': 'uint16', 'nodata': None, 'width': 7801, 'height': 7901, 'count': 1, 'crs': CRS.from_epsg(32632), 'transform': Affine(30.0, 0.0, 462885.0, 0.0, -30.0, 5531115.0)}  How to download? How to download with rasterio? As we have seen the coordinate reference system (CRS) of the Landsat GeoTIFFs is Universal Transverse Mercator (UTM) system. Our AOI bounding box is given in latitudes and longitudes. To get the data of the AOI we first need to derive the bounding box coordinates in the CRS of the imagery. Then we can use these coordinates to create a rasterio.windows.Window instance using the rasterio.windows.from_bounds function. Finally we are ready to read just the data we need.\nLet us converting the AOI to the imagery CRS and taking the bounding box coorinates from there.\ngdf_aoi_32632 = gdf_aoi.to_crs({\u0026#39;init\u0026#39;: \u0026#39;epsg:32632\u0026#39;}) gdf_aoi.to_file(filename=\u0026#39;data_federsee/aoi_federsee_32632.geojson\u0026#39;, driver=\u0026#39;GeoJSON\u0026#39;) print(gdf_aoi_32632.bounds.loc[0]) west_32632, south_32632, east_32632, north_32632 = \\ gdf_aoi_32632.bounds.loc[0, :] minx 5.404216e+05 miny 5.324001e+06 maxx 5.502409e+05 maxy 5.328391e+06 Name: 0, dtype: float64  Note of caution:\nThis is different from converting the two points and using the coordinates of the resulting points to create the bounding box! This is what I did first, so here is the difference.\nproj_4326 = Proj(init=\u0026#39;epsg:4326\u0026#39;) proj_32632 = Proj(init=\u0026#39;epsg:32632\u0026#39;) west_32632_INEXACT, north_32632_INEXACT = transform(proj_4326, proj_32632, west, north) east_32632_INEXACT, south_32632_INEXACT = transform(proj_4326, proj_32632, east, south) aoi_polygon_INEXACT = geometry.Polygon([[west_32632_INEXACT, north_32632_INEXACT], [east_32632_INEXACT, north_32632_INEXACT], [east_32632_INEXACT, south_32632_INEXACT], [west_32632_INEXACT, south_32632_INEXACT]]) gdf_aoi_32632.loc[1, :] = (2, \u0026#34;Federsee_INEXACT\u0026#34;, aoi_polygon_INEXACT) map_osm = folium.Map() map_osm.fit_bounds([[south, west], [south + .01, west + .01]]) folium.GeoJson(gdf_aoi_32632).add_to(map_osm) map_osm  We can easily show this - see how the lower right and upper left corners are different. So let us delete what we do not need.\ndel west_32632_INEXACT, north_32632_INEXACT, east_32632_INEXACT, south_32632_INEXACT, aoi_polygon_INEXACT gdf_aoi_32632 = gdf_aoi_32632.drop(1) With the correct window we new derive a window aligned with the pixels such that the AOI and that the pixels of the raster subset we derive are exactly aligned with the source raster. If we would not do that, there would be a small shift between the pixel edges.\nwindow = rasterio.windows.from_bounds(west_32632, south_32632, east_32632, north_32632, transform=meta[\u0026#34;transform\u0026#34;] ) window_aligned = rasterio.windows.Window(col_off=np.floor(window.col_off), row_off=np.floor(window.row_off), width=np.ceil(window.width) + 1, height=np.ceil(window.height) + 1) # compare pd.concat([pd.Series(window.todict(), name=\u0026#34;window_original\u0026#34;), pd.Series(window_aligned.todict(), name=\u0026#34;window_aligned\u0026#34;)], axis=1)  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  window_original window_aligned     col_off 2584.554036 2584.0   row_off 6757.478236 6757.0   width 327.310592 329.0   height 146.328742 148.0     Now we can read the data of our AOI directly in Python. The code how to safe the data to disc is also there but commented out since we do not want to do this here but with the gdal_translate solution shown below.\n# fp_dst = Path(\u0026#34;./data_federsee/dev/\u0026#34;) / (Path(fp).stem + \u0026#34;_AOI_DEV_rasterio.tif\u0026#34;) # fp_dst.parent.mkdir(exist_ok=True, parents=True) with rasterio.open(fp) as src: arr = src.read(window=window_aligned) # kwargs = src.meta.copy() # kwargs.update({ # \u0026#39;height\u0026#39;: window_aligned.height, # \u0026#39;width\u0026#39;: window_aligned.width, # \u0026#39;transform\u0026#39;: rasterio.windows.transform(window_aligned,  # src.transform)}) # with rasterio.open(fp_dst, \u0026#39;w\u0026#39;, **kwargs) as dst: # dst.write(src.read(window=window_aligned)) import seaborn as sns plt.figure(figsize=(30,15)) plt.imshow(arr[0]) plt.show() How to download with GDAL? If it is not necessary to directly load the data in Python it might be easier to use teh comman line tool gdal_translate to store the raster subset locally. For that we need the following parameters.\ngdal_translate [-projwin ulx uly lrx lry] [-projwin_srs srs_def] src_dataset dst_dataset Note of caution again:\nAs above, by using the AOI in the geographic coordinate system we will get an output raster which does not fully cover the AOI. Therefore, we use the AOI in the imagery CRS and expand it for the pixel size of 30 m.\nLet us define a function which creates the GDAL command and optionally runs it.\ndef gdal_translate_landsat_pds_c1_l8(scene: str, band: str, gdf_aoi: gpd.GeoDataFrame, dstdir: Path or str, process: bool=False, overwrite: bool=False, verbose: bool=True): \u0026#34;\u0026#34;\u0026#34;Parse a gdal_translate command to save a subset of a aws landsat 8 pds c1 raster dataset locally.\u0026#34;\u0026#34;\u0026#34; path = scene.split(\u0026#34;_\u0026#34;)[2][:3] row = scene.split(\u0026#34;_\u0026#34;)[2][3:] src_dataset = f\u0026#34;/vsicurl/http://landsat-pds.s3.amazonaws.com/c1/L8/\u0026#34; \\ f\u0026#34;{path}/{row}/{scene}/{scene}_{band}.TIF\u0026#34; dst_dataset = Path(dstdir) / scene / (f\u0026#34;{scene}_{band}.TIF\u0026#34;) dst_dataset.parent.mkdir(exist_ok=True, parents=True) # in case the aoi gdf has more features we use the bounding box embracing all projwin = pd.concat([gdf_aoi.bounds[[\u0026#34;minx\u0026#34;, \u0026#34;miny\u0026#34;]].min(), gdf_aoi.bounds[[\u0026#34;maxx\u0026#34;, \u0026#34;maxy\u0026#34;]].max()]) \\ .rename({\u0026#34;minx\u0026#34;: \u0026#34;ulx\u0026#34;, \u0026#34;miny\u0026#34;: \u0026#34;lry\u0026#34;, \u0026#34;maxx\u0026#34;: \u0026#34;lrx\u0026#34;, \u0026#34;maxy\u0026#34;: \u0026#34;uly\u0026#34;}) cmd = \u0026#34;gdal_translate \u0026#34; \\ f\u0026#34;-projwin {projwin[\u0026#39;ulx\u0026#39;]} {projwin[\u0026#39;uly\u0026#39;]} {projwin[\u0026#39;lrx\u0026#39;] + 30} {projwin[\u0026#39;lry\u0026#39;] - 30} \u0026#34; \\ f\u0026#34;{str(src_dataset)} \u0026#34; \\ f\u0026#34;{str(dst_dataset)}\u0026#34; exit_code = np.nan if process: if overwrite or not dst_dataset.exists(): if verbose: print(\u0026#34;PROCESSING\u0026#34;) exit_code = subprocess.check_call(cmd, shell=True) else: if verbose: print(f\u0026#34;Processing skipped. File exists: {str(dst_dataset)}\u0026#34;) print(cmd) return cmd, exit_code An example command looks as follows:\ncmd, exit_code = gdal_translate_landsat_pds_c1_l8(scene=scenes[0], band=\u0026#34;B5\u0026#34;, gdf_aoi=gdf_aoi_32632, dstdir=\u0026#34;./data_federsee/dev\u0026#34;, process=False, overwrite=False, verbose=True) cmd 'gdal_translate -projwin 540421.6210704312 5328390.652911871 550270.9388222749 5323970.790656615 /vsicurl/http://landsat-pds.s3.amazonaws.com/c1/L8/194/026/LC08_L1TP_194026_20190412_20190412_01_RT/LC08_L1TP_194026_20190412_20190412_01_RT_B5.TIF data_federsee/dev/LC08_L1TP_194026_20190412_20190412_01_RT/LC08_L1TP_194026_20190412_20190412_01_RT_B5.TIF'  Download Remember the task - we want to download the bands 2, 3, 4, 5, 6 and 7 of the three scenes above but only the parts that overlap with the AOI.\nNow we have everything together such that this task can be solved by a very simple for-loop.\nfor i, scene in enumerate(scenes): print(f\u0026#34;{scene} - {i+1} / {len(scenes)}\u0026#34;) for band in tqdm(bands, total=len(bands)): cmd, exit_code = gdal_translate_landsat_pds_c1_l8(scene=scene, band=band, gdf_aoi=gdf_aoi_32632, dstdir=\u0026#34;./data_federsee/l8_aoi\u0026#34;, process=True, overwrite=False, verbose=False)  0%| | 0/7 [00:00\u0026lt;?, ?it/s] LC08_L1TP_194026_20190412_20190412_01_RT - 1 / 3 100%|██████████| 7/7 [00:47\u0026lt;00:00, 6.79s/it] 0%| | 0/7 [00:00\u0026lt;?, ?it/s] LC08_L1TP_194026_20190818_20190818_01_RT - 2 / 3 100%|██████████| 7/7 [00:48\u0026lt;00:00, 6.91s/it] 0%| | 0/7 [00:00\u0026lt;?, ?it/s] LC08_L1TP_194026_20190903_20190903_01_RT - 3 / 3 100%|██████████| 7/7 [00:48\u0026lt;00:00, 7.03s/it]  The end Great, we have downloaded the first ingredient required for the classification task to be solve in the third post of this series. In the next post we will get some georeferenced land use and land cover labels for the same area from OpenStreetMap.\nAt the same time I have learned some things about Cloud Optimized GeoTIFFs. Particularly that it can save a lot of web traffic if we are only interested in a small part of raster datasets that cover much larger footprints. For example here we ended up downloadint 2MB. If we hat to downloaded the full three Landsat scene that would have summed up to around 3 x 900 MB and quite some more lines of code to get the data subset we wanted.\nSo in my opinion Cloud Optimized GeoTIFF is really an amazing raster format which we will see and use more and more in the future. In fact, the Collection 2 Level-2 Landsat data will be realeased in COG format by the U.S. Geological Survey (USGS). Which is great.\nFeedback You are more than welcome to leave a comment if you have some thoughts you like to share.\nI would love to hear what you liked, what you missed, where you agree or disagree, what you would solve in a different way or any other thoughts.\n",
    "ref": "/blog/2019-09-29-1_federsee-blog-series_part-1_cog/"
  },{
    "title": "Hi, I am Ben.",
    "date": "",
    "description": "Read more in paragraphs full of I, My and Me.",
    "body": "My name is Benjamin Mack. I grew up in Swabia in south west Germany.\nI did my aternative service in lieu of military service in Solingen. Then I lived in Bonn for many years, interrupted by one funny ERASMUS year in Granada, Spain. After one year in Karlsruhe I came to Munich where I live ever since.\nI studied in Bonn having the subjects geography, ethnology and economics. In the end I discovered and was facinated by Remote Sensing and Earth Observation. After my studies I spent several years doing research and developement at universities, research centers and companies. Surprisingly for myself now I work in a large reinsurance company.\nI am interested in and fascinated by data. Processing and analyzing it is what I am exited about. I love to write code - in my early years at university mainly in R and since several years mostly in Python.\nIn my free time I enjoy cooking, nature, biking, reading, photography, traveling, the italina language and of course spending my time with amazing people.\n",
    "ref": "/about/"
  },{
    "title": "Projects on GitHub",
    "date": "",
    "description": "",
    "body": "Selected projects from my GitHub profile.\neobox: A Python package for processing earth observation data.\nnasa_hls: A Python package to download data from NASA\u0026rsquo;s Harmonized Landsat and Sentinel-2 project.\nclassify-hls: A Python project to showcase the classification of HLS Sentinel-2/Landsat data (multple tiles) with reference data from CORINE Land Use / Land Cover data using nasa_hls and eobox.\noneClass: An R package for one-class classification which I have developed during my PhD.\nlucas: An R package for downloading and processing Land cover/use statistics (LUCAS) data provided by EUROSTAT.\n",
    "ref": "/projects/"
  },{
    "title": "YAY...",
    "date": "",
    "description": "... my first blog post!",
    "body": "A journey of a thousand miles begins with a single step. - Tao Te Ching\n",
    "ref": "/blog/2019-09-28-1_yay/"
  },{
    "title": "Contact",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/contact/"
  }]
